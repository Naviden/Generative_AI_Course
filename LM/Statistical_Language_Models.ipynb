{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b88ffa75",
   "metadata": {},
   "source": [
    "# Statistical Language Models\n",
    "\n",
    "This notebook provides an overview and examples of three types of statistical language models: n-grams, Hidden Markov Models (HMMs), and Maximum Likelihood Estimation (MLE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c10c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Alice.txt', 'r') as file:\n",
    "    corpus = file.readlines()#.split('.')\n",
    "A_corpus = [x.strip() for x in corpus if x != '\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3174f6f",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "An n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application. N-grams are used in various areas of statistical natural language processing and genetic sequence analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0941c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word after 'the': cat\n",
      "Next word after 'the dog': sat\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self):\n",
    "        # Initialize dictionaries to store bigram and trigram counts\n",
    "        self.bigrams = defaultdict(Counter)\n",
    "        self.trigrams = defaultdict(Counter)\n",
    "        self.starting_bigrams = Counter()\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the language model using the provided corpus.\n",
    "        The corpus should be a list of sentences.\n",
    "\n",
    "        :param corpus: List of sentences (strings).\n",
    "        \"\"\"\n",
    "        for sentence in corpus:\n",
    "            # Tokenize the sentence with special tokens <start> and <end>\n",
    "            tokens = ['<start>'] + sentence.split() + ['<end>']\n",
    "            for i in range(len(tokens) - 2):\n",
    "                # Count bigrams and trigrams\n",
    "                self.bigrams[tokens[i]][tokens[i + 1]] += 1\n",
    "                self.trigrams[(tokens[i], tokens[i + 1])][tokens[i + 2]] += 1\n",
    "                # Track starting bigrams for sentence generation\n",
    "                if i == 0:\n",
    "                    self.starting_bigrams[tokens[i]] += 1\n",
    "\n",
    "    def predict_next_word(self, word_sequence):\n",
    "        \"\"\"\n",
    "        Predict the next word based on the given word sequence using the trained model.\n",
    "        It first tries using the trigram model and then falls back to the bigram model.\n",
    "\n",
    "        :param word_sequence: A string of word sequence.\n",
    "        :return: The predicted next word, or None if no prediction is available.\n",
    "        \"\"\"\n",
    "        tokens = word_sequence.split()\n",
    "        # Check for trigram match\n",
    "        if len(tokens) >= 2 and (tokens[-2], tokens[-1]) in self.trigrams:\n",
    "            return self.trigrams[(tokens[-2], tokens[-1])].most_common(1)[0][0]\n",
    "        # Fallback to bigram match\n",
    "        elif tokens[-1] in self.bigrams:\n",
    "            return self.bigrams[tokens[-1]].most_common(1)[0][0]\n",
    "        else:\n",
    "            # Return None if no match is found\n",
    "            return None\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the rug\",\n",
    "    \"the dog played with the cat\",\n",
    "    \"the cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "model = NGramLanguageModel()\n",
    "model.train(corpus)\n",
    "\n",
    "# Predict the next word for bigram sequence\n",
    "bigram_sequence = \"the\"\n",
    "next_word_bigram = model.predict_next_word(bigram_sequence)\n",
    "print(f\"Next word after '{bigram_sequence}': {next_word_bigram}\")\n",
    "\n",
    "# Predict the next word for trigram sequence\n",
    "trigram_sequence = \"the dog\"\n",
    "next_word_trigram = model.predict_next_word(trigram_sequence)\n",
    "print(f\"Next word after '{trigram_sequence}': {next_word_trigram}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c982b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word after 'Alice': was\n"
     ]
    }
   ],
   "source": [
    "model = NGramLanguageModel()\n",
    "model.train(A_corpus)\n",
    "\n",
    "# Predict the next word for bigram sequence\n",
    "bigram_sequence = \"Alice\"\n",
    "next_word_bigram = model.predict_next_word(bigram_sequence)\n",
    "print(f\"Next word after '{bigram_sequence}': {next_word_bigram}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771cc1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was a little pattering of the White Rabbit blew three gardeners at the White Rabbit blew three gardeners at the White Rabbit blew three gardeners at the White Rabbit blew three gardeners at the White Rabbit blew three gardeners at the White Rabbit blew three gardeners at the White Rabbit\n"
     ]
    }
   ],
   "source": [
    "model = NGramLanguageModel()\n",
    "model.train(A_corpus[200:1000])\n",
    "\n",
    "res = []\n",
    "word = 'Alice'\n",
    "res.append(word)\n",
    "for i in range(50):\n",
    "    word = model.predict_next_word(word)\n",
    "    res.append(word)\n",
    "print(' '.join(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb6f03",
   "metadata": {},
   "source": [
    "# Reasons for Repetitive Output in N-Gram Language Models\n",
    "\n",
    "When using basic n-gram models, particularly bigrams, it's common to encounter outputs that get stuck in repetitive loops. Several factors contribute to this behavior:\n",
    "\n",
    "## Strong Affinity Between Words\n",
    "\n",
    "If the training corpus contains frequently repeating phrases, the model might learn a strong association between certain word pairs. For instance, if \"the Queen said\" occurs often, the model learns that \"said\" is likely to follow \"Queen.\" This can cause a feedback loop where the model continually predicts the same sequence.\n",
    "\n",
    "## Limited Context\n",
    "\n",
    "Bigram models consider only the previous word to predict the next one. This limited context can lead to loops if a word pair like \"Queen said\" is commonly followed by \"the,\" and \"the\" is commonly followed by \"Queen\" again.\n",
    "\n",
    "## Sparse Data\n",
    "\n",
    "Training on a small subset of the corpus may not expose the model to enough variety in word sequences, leading to overfitting on certain patterns. The model's output then reflects these overrepresented patterns.\n",
    "\n",
    "## Lack of Smoothing\n",
    "\n",
    "Without smoothing, n-gram models assign zero probability to unseen n-grams. This can lead to repetitive predictions since the model tends to choose the highest probability next word it has encountered during training.\n",
    "\n",
    "## End-of-Sequence Handling\n",
    "\n",
    "Simple n-gram models may not have a clear indication of when to end a sentence, leading to run-on sentences or repetitive loops, as they lack an understanding of natural sentence length variation.\n",
    "\n",
    "## Solutions\n",
    "\n",
    "To mitigate the repetition in generated text:\n",
    "\n",
    "- Move to higher-order n-grams (trigrams, 4-grams, etc.) to provide more context for predictions.\n",
    "- Implement smoothing techniques to better handle unseen word pairs.\n",
    "- Use a larger and more diverse training corpus to provide a broader learning base for the model.\n",
    "- Introduce mechanisms to detect and prevent loops in generated text, such as setting a maximum sentence length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b59be6",
   "metadata": {},
   "source": [
    "## Hidden Markov Models (HMMs)\n",
    "\n",
    "A Hidden Markov Model is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. HMMs can be considered as the simplest dynamic Bayesian networks. In a regular Markov model, the state is directly visible to the observer, and therefore the state transition probabilities are the only parameters, while in a hidden Markov model, the state is not directly visible, but the output, dependent on the state, is visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "039a1e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat are friends and are friends and are friends and\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class HMMWordGenerator:\n",
    "    def __init__(self, n_states=3):\n",
    "        \"\"\"\n",
    "        Initialize the HMMWordGenerator with a specified number of states.\n",
    "\n",
    "        :param n_states: Number of hidden states in the HMM.\n",
    "        \"\"\"\n",
    "        self.model = hmm.MultinomialHMM(n_components=n_states, random_state=42)\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, sentences):\n",
    "        \"\"\"\n",
    "        Train the HMM model on a given corpus.\n",
    "\n",
    "        :param sentences: A list of sentences for training the model.\n",
    "        \"\"\"\n",
    "        # Extract unique words from the sentences\n",
    "        unique_words = set(word for sentence in sentences for word in sentence.split())\n",
    "        # Fit the label encoder with the unique words\n",
    "        self.label_encoder.fit(list(unique_words))\n",
    "\n",
    "        # Transform sentences to sequences of numerical labels\n",
    "        encoded_sequences = [self.label_encoder.transform(sentence.split()) for sentence in sentences]\n",
    "        # Calculate the lengths of each sentence\n",
    "        lengths = [len(seq) for seq in encoded_sequences]\n",
    "        # Combine all sequences into a single NumPy array for HMM training\n",
    "        X = np.concatenate(encoded_sequences).reshape(-1, 1)\n",
    "\n",
    "        # Fit the HMM model to the data\n",
    "        self.model.fit(X, lengths=lengths)\n",
    "\n",
    "    def sample(self, start_word, max_length=10):\n",
    "        \"\"\"\n",
    "        Generate a sequence of words starting with the given word.\n",
    "\n",
    "        :param start_word: The word to start the sequence.\n",
    "        :param max_length: Maximum length of the generated sequence.\n",
    "        :return: A string representing the generated sequence.\n",
    "        \"\"\"\n",
    "        # Encode the starting word\n",
    "        start_word_encoded = self.label_encoder.transform([start_word])[0]\n",
    "        word_sequence = [start_word_encoded]\n",
    "\n",
    "        # Generate the sequence\n",
    "        for _ in range(max_length - 1):\n",
    "            current_state = word_sequence[-1]\n",
    "            # Check if the current state is within the bounds of the transition matrix\n",
    "            if current_state < self.model.transmat_.shape[0]:\n",
    "                # Choose the next word based on the transition probabilities\n",
    "                next_word_prob = self.model.transmat_[current_state]\n",
    "                next_word_encoded = np.random.choice(len(next_word_prob), p=next_word_prob)\n",
    "                word_sequence.append(next_word_encoded)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Decode the sequence back to words\n",
    "        return ' '.join(self.label_encoder.inverse_transform(word_sequence))\n",
    "\n",
    "# Example usage\n",
    "sentences = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the rug\",\n",
    "    \"the dog played with the cat\",\n",
    "    \"the cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "# Initialize and train the HMM model\n",
    "hmm_model = HMMWordGenerator(n_states=5)\n",
    "hmm_model.fit(sentences)\n",
    "\n",
    "# Generate a sequence starting with 'the'\n",
    "print(hmm_model.sample('cat'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9e76a",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Maximum Likelihood Estimation (MLE) is a method of estimating the parameters of a statistical model. In the context of language models, MLE is used to estimate the probabilities of different words (or sequences of words) occurring. The idea is to choose the parameters of a model in such a way that the likelihood of the observed data is maximized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe22b16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "sat\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class MLELanguageModel:\n",
    "    def __init__(self):\n",
    "        # Dictionary to store bigram counts\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        # Dictionary to store unigram counts\n",
    "        self.unigram_counts = Counter()\n",
    "\n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the language model on a given corpus.\n",
    "        \n",
    "        :param corpus: A list of sentences (strings) for training the model.\n",
    "        \"\"\"\n",
    "        for sentence in corpus:\n",
    "            tokens = sentence.split()\n",
    "            # Update unigram and bigram counts\n",
    "            for i in range(len(tokens)):\n",
    "                self.unigram_counts[tokens[i]] += 1\n",
    "                if i < len(tokens) - 1:\n",
    "                    self.bigram_counts[tokens[i]][tokens[i + 1]] += 1\n",
    "\n",
    "    def predict_next_word(self, word):\n",
    "        \"\"\"\n",
    "        Predict the next word given a word using the trained model.\n",
    "        \n",
    "        :param word: The input word to base the prediction on.\n",
    "        :return: The predicted next word.\n",
    "        \"\"\"\n",
    "        if word not in self.bigram_counts:\n",
    "            return None\n",
    "        # Predict the next word based on maximum likelihood estimation\n",
    "        return self.bigram_counts[word].most_common(1)[0][0]\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the rug\",\n",
    "    \"the dog played with the cat\",\n",
    "    \"the cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "model = MLELanguageModel()\n",
    "model.train(corpus)\n",
    "\n",
    "# Predict the next word for 'the'\n",
    "print(model.predict_next_word('the'))\n",
    "\n",
    "# Predict the next word for 'cat'\n",
    "print(model.predict_next_word('cat'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "598d37b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was a little golden key, and the Project Gutenberg™ electronic works in the Project Gutenberg™ electronic works in the Project Gutenberg™ electronic works in the Project Gutenberg™ electronic works in the Project Gutenberg™ electronic works in the Project Gutenberg™ electronic works in the Project Gutenberg™ electronic works in the Project\n"
     ]
    }
   ],
   "source": [
    "model = MLELanguageModel()\n",
    "model.train(A_corpus)\n",
    "\n",
    "res = []\n",
    "word = 'Alice'\n",
    "res.append(word)\n",
    "for i in range(50):\n",
    "    word = model.predict_next_word(word)\n",
    "    res.append(word)\n",
    "print(' '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "268fd9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was a little timidly, \"why your temper,\" said the same thing is a little timidly, \"why your temper,\" said the same thing is a little timidly, \"why your temper,\" said the same thing is a little timidly, \"why your temper,\" said the same thing is a little timidly, \"why your\n"
     ]
    }
   ],
   "source": [
    "model = MLELanguageModel()\n",
    "model.train(A_corpus[500:800])\n",
    "\n",
    "res = []\n",
    "word = 'Alice'\n",
    "res.append(word)\n",
    "for i in range(50):\n",
    "    word = model.predict_next_word(word)\n",
    "    res.append(word)\n",
    "print(' '.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c22f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1aeac71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn-crfsuite\n",
      "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
      "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite)\n",
      "  Downloading python-crfsuite-0.9.9.tar.gz (440 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/navid/anaconda3/lib/python3.11/site-packages (from sklearn-crfsuite) (1.16.0)\n",
      "Collecting tabulate (from sklearn-crfsuite)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: tqdm>=2.0 in /Users/navid/anaconda3/lib/python3.11/site-packages (from sklearn-crfsuite) (4.65.0)\n",
      "Building wheels for collected packages: python-crfsuite\n",
      "  Building wheel for python-crfsuite (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-crfsuite: filename=python_crfsuite-0.9.9-cp311-cp311-macosx_11_0_arm64.whl size=145221 sha256=a966ab3486e892c23a6f2c98cc433bc348e3e1369ef805d2d9d1596117f6b724\n",
      "  Stored in directory: /Users/navid/Library/Caches/pip/wheels/4a/23/ab/586db2b4846c6de75693dec052c3cfc77e3c920f6a9ba97342\n",
      "Successfully built python-crfsuite\n",
      "Installing collected packages: python-crfsuite, tabulate, sklearn-crfsuite\n",
      "Successfully installed python-crfsuite-0.9.9 sklearn-crfsuite-0.3.6 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install sklearn-crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77618126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "class CRFLanguageModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the CRF-based language model.\n",
    "        \"\"\"\n",
    "        self.model = sklearn_crfsuite.CRF(\n",
    "            algorithm='lbfgs',\n",
    "            c1=0.1,\n",
    "            c2=0.1,\n",
    "            max_iterations=100,\n",
    "            all_possible_transitions=True\n",
    "        )\n",
    "    \n",
    "    def _create_features(self, sentence, index):\n",
    "        \"\"\"\n",
    "        Create feature dict for a given word in a sentence.\n",
    "        \n",
    "        :param sentence: List of words in the sentence.\n",
    "        :param index: Index of the word to create features for.\n",
    "        :return: A dict of features.\n",
    "        \"\"\"\n",
    "        word = sentence[index]\n",
    "        features = {\n",
    "            'word': word,\n",
    "            'is_first': index == 0,\n",
    "            'is_last': index == len(sentence) - 1,\n",
    "            'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "            'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "            # More features can be added here\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _prepare_data(self, corpus):\n",
    "        \"\"\"\n",
    "        Prepare training data for CRF, extracting features and labels.\n",
    "        \n",
    "        :param corpus: List of sentences to train on.\n",
    "        :return: A tuple (X_train, y_train) for features and labels.\n",
    "        \"\"\"\n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for sentence in corpus:\n",
    "            words = sentence.split()\n",
    "            X_sentence = [self._create_features(words, i) for i in range(len(words))]\n",
    "            y_sentence = words[1:] + ['END']\n",
    "            X_train.append(X_sentence)\n",
    "            y_train.append(y_sentence)\n",
    "        return X_train, y_train\n",
    "    \n",
    "    def train(self, corpus):\n",
    "        \"\"\"\n",
    "        Train the CRF language model on the given corpus.\n",
    "        \n",
    "        :param corpus: List of sentences to train on.\n",
    "        \"\"\"\n",
    "        X_train, y_train = self._prepare_data(corpus)\n",
    "        self.model.fit(X_train, y_train)\n",
    "    \n",
    "    def generate(self, sequence, max_length=10):\n",
    "        \"\"\"\n",
    "        Generate a continuation of the given word sequence using the trained model.\n",
    "        \n",
    "        :param sequence: The starting sequence of words.\n",
    "        :param max_length: The maximum length of the continuation.\n",
    "        :return: Generated sequence of words.\n",
    "        \"\"\"\n",
    "        sentence = sequence.split()\n",
    "        for _ in range(max_length):\n",
    "            features = [self._create_features(sentence, len(sentence) - 1)]\n",
    "            next_word = self.model.predict_single(features)[0]\n",
    "            if next_word == 'END':\n",
    "                break\n",
    "            sentence.append(next_word)\n",
    "        return ' '.join(sentence)\n",
    "\n",
    "# Example usage:\n",
    "corpus = [\n",
    "    'the cat sat on the mat',\n",
    "    'the dog played with the cat',\n",
    "    'the quick brown fox jumps over the lazy dog',\n",
    "    # ... more sentences\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "19e34ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Produced', 'by', 'Jason', 'Isbell', ',', 'Irma', 'Spehar', ',', 'and', 'the', 'Online', 'Distributed', 'Proofreading', 'Team', 'at', 'http', ':', '//www.pgdp.net', '[', 'Illustration', ':', 'Alice', 'in', 'the', 'Room', 'of', 'the', 'Duchess', '.', ']']\n",
      "['_THE', '``', 'STORYLAND', \"''\", 'SERIES_', 'ALICE', \"'S\", 'ADVENTURES', 'IN', 'WONDERLAND', 'SAM', \"'\", 'L', 'GABRIEL', 'SONS', '&', 'COMPANY', 'NEW', 'YORK', 'Copyright', ',', '1916', ',', 'by', 'SAM', \"'\", 'L', 'GABRIEL', 'SONS', '&', 'COMPANY', 'NEW', 'YORK', 'ALICE', \"'S\", 'ADVENTURES', 'IN', 'WONDERLAND', '[', 'Illustration', ']', 'I', '--', 'DOWN', 'THE', 'RABBIT-HOLE', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', '.']\n",
      "['Once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', '``', 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', ',', \"''\", 'thought', 'Alice', ',', '``', 'without', 'pictures', 'or', 'conversations', '?', \"''\"]\n",
      "['So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', ')', ',', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy-chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.']\n",
      "['There', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', ',', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', ',', '``', 'Oh', 'dear', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/navid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Ensure that NLTK's tokenizers are available\n",
    "nltk.download('punkt')\n",
    "\n",
    "def read_book(file_path):\n",
    "    \"\"\"\n",
    "    Read the book text from a file.\n",
    "    \n",
    "    :param file_path: Path to the book text file.\n",
    "    :return: Raw text of the book.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Perform initial text cleaning on the book text.\n",
    "    \n",
    "    :param text: Raw text of the book.\n",
    "    :return: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove any Project Gutenberg-specific headers or footers, if present\n",
    "    start_pattern = r\"\\*\\*\\* START OF (THE|THIS) PROJECT GUTENBERG EBOOK .+ \\*\\*\\*\"\n",
    "    end_pattern = r\"\\*\\*\\* END OF (THE|THIS) PROJECT GUTENBERG EBOOK .+ \\*\\*\\*\"\n",
    "    start_match = re.search(start_pattern, text)\n",
    "    end_match = re.search(end_pattern, text)\n",
    "\n",
    "    # If start and end patterns are found, extract the main content\n",
    "    if start_match and end_match:\n",
    "        text = text[start_match.end():end_match.start()]\n",
    "\n",
    "    # Replace multiple whitespace with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_for_crf(text):\n",
    "    \"\"\"\n",
    "    Preprocess the cleaned text for CRF, including tokenization.\n",
    "    \n",
    "    :param text: Cleaned text of the book.\n",
    "    :return: List of tokenized sentences.\n",
    "    \"\"\"\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize each sentence into words\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "# Path to your text file\n",
    "file_path = 'Alice.txt'\n",
    "\n",
    "# Read and preprocess the book text\n",
    "raw_text = read_book(file_path)\n",
    "cleaned_text = clean_text(raw_text)\n",
    "tokenized_corpus = preprocess_for_crf(cleaned_text)\n",
    "\n",
    "# Now tokenized_corpus can be used to create features for CRF\n",
    "# Example of printing the first few preprocessed sentences\n",
    "for sentence in tokenized_corpus[:5]:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "37c093c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The numbers of items and labels differ: |x| = 0, |y| = 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m [sentence\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m CRFLanguageModel()\n\u001b[0;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(data)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Generate a sequence starting with 'the quick brown fox'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAlice was\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[64], line 61\u001b[0m, in \u001b[0;36mCRFLanguageModel.train\u001b[0;34m(self, corpus)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03mTrain the CRF language model on the given corpus.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m:param corpus: List of sentences to train on.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_data(corpus)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn_crfsuite/estimator.py:314\u001b[0m, in \u001b[0;36mCRF.fit\u001b[0;34m(self, X, y, X_dev, y_dev)\u001b[0m\n\u001b[1;32m    311\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m tqdm(train_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading training data to CRFsuite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xseq, yseq \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m--> 314\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mappend(xseq, yseq)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mpycrfsuite/_pycrfsuite.pyx:312\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer.append\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The numbers of items and labels differ: |x| = 0, |y| = 1"
     ]
    }
   ],
   "source": [
    "data = cleaned_text.split('.')\n",
    "data = [sentence.encode('utf-8') for sentence in data]\n",
    "\n",
    "model = CRFLanguageModel()\n",
    "model.train(data)\n",
    "\n",
    "# Generate a sequence starting with 'the quick brown fox'\n",
    "print(model.generate('Alice was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "183773ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The numbers of items and labels differ: |x| = 0, |y| = 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     X_train\u001b[38;5;241m.\u001b[39mappend(X_sentence)\n\u001b[1;32m     35\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mappend(y_sentence)\n\u001b[0;32m---> 37\u001b[0m crf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn_crfsuite/estimator.py:314\u001b[0m, in \u001b[0;36mCRF.fit\u001b[0;34m(self, X, y, X_dev, y_dev)\u001b[0m\n\u001b[1;32m    311\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m tqdm(train_data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading training data to CRFsuite\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xseq, yseq \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m--> 314\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mappend(xseq, yseq)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32mpycrfsuite/_pycrfsuite.pyx:312\u001b[0m, in \u001b[0;36mpycrfsuite._pycrfsuite.BaseTrainer.append\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The numbers of items and labels differ: |x| = 0, |y| = 1"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "import codecs\n",
    "\n",
    "\n",
    "\n",
    "def create_features(sentence, index):\n",
    "    \"\"\"\n",
    "    Create feature dict for a given word in a sentence.\n",
    "\n",
    "    :param sentence: List of words in the sentence.\n",
    "    :param index: Index of the word to create features for.\n",
    "    :return: A dict of features.\n",
    "    \"\"\"\n",
    "    word = sentence[index]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        # More features can be added here\n",
    "    }\n",
    "    return features\n",
    "\n",
    "\n",
    "crf = CRF(algorithm='lbfgs', c1=0.1, c2=0.1, max_iterations=100)\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for sentence in data:\n",
    "    words = sentence.split()\n",
    "    X_sentence = [create_features(words, i) for i in range(len(words))]\n",
    "    y_sentence = words[1:] + ['END']\n",
    "    X_train.append(X_sentence)\n",
    "    y_train.append(y_sentence)\n",
    "\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5f4243d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7145112c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87015f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
