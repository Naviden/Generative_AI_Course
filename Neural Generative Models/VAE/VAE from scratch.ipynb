{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08fb2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3aa1f9",
   "metadata": {},
   "source": [
    "The following code provides a basic, function-based implementation of a VAE. Each function is commented to describe its purpose and how it contributes to the VAE model. In the last cell, the __train_step__ function ties everything together, showing how the data flows through the encoder, sampling process, and decoder, and how the loss is calculated. Remember, this is a simplified version for educational purposes, and in practice, you'd use more complex models and libraries for real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28fc9c",
   "metadata": {},
   "source": [
    "## Initializing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec31f9e",
   "metadata": {},
   "source": [
    "When initializing the weights and biases for the encoder and decoder in a Variational Autoencoder, we typically draw the initial weights from a normal distribution and set the biases to zero. Let $ \\mathbf{W}_{\\text{enc}} $ and $ \\mathbf{W}_{\\text{dec}} $ represent the weight matrices for the encoder and decoder, respectively, and $ \\mathbf{b}_{\\text{enc}} $ and $ \\mathbf{b}_{\\text{dec}} $ represent the bias vectors. The dimensions of these matrices and vectors are determined by the dimensions of the input data and the latent space:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{\\text{enc}} \\sim \\mathcal{N}(0, 1), \\quad \\mathbf{W}_{\\text{enc}} \\in \\mathbb{R}^{\\text{input dim} \\times \\text{latentdim}}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{b}_{\\text{enc}} = \\mathbf{0}, \\quad \\mathbf{b}_{\\text{enc}} \\in \\mathbb{R}^{\\text{latent dim}}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{W}_{\\text{dec}} \\sim \\mathcal{N}(0, 1), \\quad \\mathbf{W}_{\\text{dec}} \\in \\mathbb{R}^{\\text{latent dim} \\times \\text{input dim}}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{b}_{\\text{dec}} = \\mathbf{0}, \\quad \\mathbf{b}_{\\text{dec}} \\in \\mathbb{R}^{\\text{input dim}}\n",
    "$$\n",
    "\n",
    "This initialization process is an important step in setting up a neural network before starting the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729199d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(input_dim, latent_dim):\n",
    "    \"\"\"\n",
    "    Initialize the weights and biases for the encoder and decoder.\n",
    "\n",
    "    :param input_dim: Dimensionality of the input data.\n",
    "    :param latent_dim: Dimensionality of the latent space.\n",
    "    :return: A tuple of weights and biases (enc_w, enc_b, dec_w, dec_b).\n",
    "    \"\"\"\n",
    "    enc_w = np.random.randn(input_dim, latent_dim)\n",
    "    enc_b = np.zeros(latent_dim)\n",
    "    dec_w = np.random.randn(latent_dim, input_dim)\n",
    "    dec_b = np.zeros(input_dim)\n",
    "    return enc_w, enc_b, dec_w, dec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcbcc6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784  # Example input dimension\n",
    "latent_dim = 2   # Example latent dimension\n",
    "enc_w, enc_b, dec_w, dec_b = initialize_weights(input_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6fa408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80233873,  0.56551687],\n",
       "       [ 0.05659242,  1.03044558],\n",
       "       [ 0.03309176,  0.5305171 ],\n",
       "       ...,\n",
       "       [ 0.50934032, -0.60992901],\n",
       "       [-1.59743326, -0.51895062],\n",
       "       [ 0.7940917 ,  0.9068555 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd5ad89",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498fc48",
   "metadata": {},
   "source": [
    "The encoder in a Variational Autoencoder maps the input data $ \\mathbf{x} $ to the parameters of a latent distribution. Specifically, it computes the mean $ \\boldsymbol{\\mu} $ and standard deviation $ \\boldsymbol{\\sigma} $ of the latent space distribution. The mean is computed as a linear transformation of the input data with a weight matrix $ \\mathbf{W}_{\\text{enc}} $ and a bias vector $ \\mathbf{b}_{\\text{enc}} $. For simplicity, the standard deviation is assumed to be fixed:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\mathbf{x} \\mathbf{W}_{\\text{enc}} + \\mathbf{b}_{\\text{enc}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\sigma} = \\mathbf{1}\n",
    "$$\n",
    "\n",
    "Where $ \\mathbf{1} $ denotes a vector of ones with the appropriate dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06aa1ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(x, enc_w, enc_b):\n",
    "    \"\"\"\n",
    "    The encoder function that maps the input to the latent space.\n",
    "\n",
    "    :param x: Input data.\n",
    "    :param enc_w: Encoder weights.\n",
    "    :param enc_b: Encoder biases.\n",
    "    :return: Mean (mu) and standard deviation (sigma) of the latent space distribution.\n",
    "    \"\"\"\n",
    "    mu = np.dot(x, enc_w) + enc_b\n",
    "    sigma = np.ones(mu.shape)  # Assuming a fixed standard deviation\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e82155",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(100, input_dim)  # Random input data\n",
    "mu, sigma = encoder(x, enc_w, enc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8314d044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-18.45172333,  31.27820171],\n",
       "       [-29.80332442,  -6.23927152],\n",
       "       [-23.50797805,  31.01750073],\n",
       "       [-51.99421183, -35.92739959],\n",
       "       [-17.99465   , -39.84129533],\n",
       "       [ -5.76375495,   8.9868113 ],\n",
       "       [ 13.9234647 ,  -7.64509702],\n",
       "       [ 21.44715055,  14.98285369],\n",
       "       [ 13.55582687,  39.58754232],\n",
       "       [  1.74272721,  -7.32442935],\n",
       "       [  9.49795085,  -9.36024668],\n",
       "       [ 34.06248475,   4.60357008],\n",
       "       [ 33.91405612,  29.53842075],\n",
       "       [  5.90391893, -22.24461306],\n",
       "       [-48.22923345,  -3.11899728],\n",
       "       [ 41.47277708,  19.73699114],\n",
       "       [ 62.05416333, -46.53936069],\n",
       "       [ 51.70999476,  -6.37074961],\n",
       "       [-31.20499674,  20.12755612],\n",
       "       [  0.71224971, -30.70985329],\n",
       "       [ 13.12195452,  27.57190414],\n",
       "       [  7.96524881,  22.99133406],\n",
       "       [ 23.75119982, -41.96028114],\n",
       "       [ 60.9280171 , -15.8103524 ],\n",
       "       [ 27.95969991,   6.2084801 ],\n",
       "       [-22.20911858,  27.17389891],\n",
       "       [-22.99638747,  13.01904148],\n",
       "       [-13.36699938,   3.83706666],\n",
       "       [-20.63799718,  18.47713527],\n",
       "       [  4.45341862,  14.43340474],\n",
       "       [ 21.56253689, -21.65795473],\n",
       "       [ 11.9228604 ,  45.32597585],\n",
       "       [  1.18009523,  36.15023529],\n",
       "       [  0.54247488, -24.04727402],\n",
       "       [ 24.45113184,  33.28814093],\n",
       "       [-14.21791691, -36.00420867],\n",
       "       [-36.33525648,  39.65272395],\n",
       "       [  7.61895436, -16.18651525],\n",
       "       [ 19.81602269,  -0.5653074 ],\n",
       "       [-36.56382998,  20.77241773],\n",
       "       [-20.71686102, -33.23572763],\n",
       "       [-80.45859736,  -3.25696671],\n",
       "       [-20.79109127, -22.89082702],\n",
       "       [-18.55588516,   1.16219911],\n",
       "       [ -2.5416127 , -99.25566239],\n",
       "       [-16.87221503, -12.67773409],\n",
       "       [ 12.71824778, -37.91694015],\n",
       "       [-23.51183237,  16.15137583],\n",
       "       [  2.66296341, -24.04209067],\n",
       "       [ -5.16819577,  51.88924916],\n",
       "       [-14.61672406, -37.10656499],\n",
       "       [ 47.93334475,  29.85426405],\n",
       "       [ 49.64429579,  19.30546381],\n",
       "       [ 30.30555113, -19.66645554],\n",
       "       [ 30.09932322, -11.8251231 ],\n",
       "       [ 45.88964444,   8.77522204],\n",
       "       [ 34.31246554,  12.4863452 ],\n",
       "       [ 24.08188126,  -4.22088585],\n",
       "       [-39.23402606,   1.30539001],\n",
       "       [  9.80836387, -23.07155895],\n",
       "       [  5.08447305,  34.59788807],\n",
       "       [ 27.30447448,  30.32630098],\n",
       "       [-17.26304182, -77.93132089],\n",
       "       [-12.89165545, -24.08164346],\n",
       "       [-20.5355263 ,  51.60042954],\n",
       "       [ 28.10396252,  -9.92631328],\n",
       "       [ -0.33871327,  -9.3353448 ],\n",
       "       [ -8.2746948 , -41.43829639],\n",
       "       [ -6.36658293,  33.92993374],\n",
       "       [ 38.9055407 , -25.89684513],\n",
       "       [ 66.03432778,  39.74962546],\n",
       "       [  2.05088958, -49.31130757],\n",
       "       [ 10.46347628, -45.38092707],\n",
       "       [ 47.26924767, -49.00907268],\n",
       "       [  7.28159043, -23.00859084],\n",
       "       [  8.23247736, -19.22913302],\n",
       "       [-19.54419958,  10.3459369 ],\n",
       "       [ 23.76002109,  18.91295319],\n",
       "       [  9.7891418 , -50.47329305],\n",
       "       [-34.48110337, -19.23812962],\n",
       "       [  7.45020433,   3.17771042],\n",
       "       [-47.64678904,  26.3702412 ],\n",
       "       [ 18.5240925 ,   3.45183983],\n",
       "       [ -5.59224595,  43.41149767],\n",
       "       [ 33.41715838,  -4.76588255],\n",
       "       [ 34.25724011,  19.77367656],\n",
       "       [ 14.37772982, -11.92984611],\n",
       "       [-27.39022518, -24.234017  ],\n",
       "       [ 14.982972  , -43.17295722],\n",
       "       [-12.59985354, -14.09488656],\n",
       "       [-21.26753034,  27.61458571],\n",
       "       [-21.77454017, -24.43769199],\n",
       "       [-17.29761864,   5.8566872 ],\n",
       "       [-13.54619541,  31.57118374],\n",
       "       [ -9.69596872, -26.49346497],\n",
       "       [ 14.75061129, -19.90973892],\n",
       "       [-14.43709704, -20.26959389],\n",
       "       [ -7.40145784,  23.58854801],\n",
       "       [ 36.02504932, -47.86640545],\n",
       "       [-17.6333527 ,  21.94971989]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15206dbf",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35424d33",
   "metadata": {},
   "source": [
    "In the VAE, a random sample $ \\boldsymbol{\\epsilon} $ is drawn from a standard normal distribution. This sample is then scaled by the standard deviation $ \\boldsymbol{\\sigma} $ and shifted by the mean $ \\boldsymbol{\\mu} $ to produce the latent variable $ \\mathbf{z} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\cdot \\boldsymbol{\\epsilon}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60ab40da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(mu, sigma):\n",
    "    \"\"\"\n",
    "    Sampling function to sample from the latent space distribution.\n",
    "\n",
    "    :param mu: Mean of the latent space distribution.\n",
    "    :param sigma: Standard deviation of the latent space distribution.\n",
    "    :return: Sampled latent variable.\n",
    "    \"\"\"\n",
    "    eps = np.random.randn(*mu.shape)\n",
    "    return mu + sigma * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8a7f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sampling(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f006cb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-17.82047493,  30.36330822],\n",
       "       [-27.60577212,  -4.82905404],\n",
       "       [-24.08659155,  30.77019172],\n",
       "       [-52.93688359, -34.86998295],\n",
       "       [-16.24090578, -40.82944641],\n",
       "       [ -6.39787977,   8.83171184],\n",
       "       [ 11.90825485,  -5.7541863 ],\n",
       "       [ 21.50929071,  15.24117752],\n",
       "       [ 12.40592017,  38.84888615],\n",
       "       [  1.52560557,  -7.03715691],\n",
       "       [  9.11640169,  -9.35686068],\n",
       "       [ 32.93685855,   3.79359377],\n",
       "       [ 34.60874374,  28.21672448],\n",
       "       [  5.97748957, -22.20837791],\n",
       "       [-48.33853286,  -1.77113827],\n",
       "       [ 41.07321072,  19.45506964],\n",
       "       [ 62.35250474, -45.5332415 ],\n",
       "       [ 50.04223778,  -6.37384827],\n",
       "       [-31.22300008,  21.17241557],\n",
       "       [  0.97923168, -29.70550656],\n",
       "       [ 13.37089585,  30.34585583],\n",
       "       [  7.86834519,  23.45927488],\n",
       "       [ 23.84546619, -41.02588206],\n",
       "       [ 63.56334017, -15.33687521],\n",
       "       [ 28.81934442,   6.594151  ],\n",
       "       [-23.66440755,  27.18166119],\n",
       "       [-22.14373362,  13.4307478 ],\n",
       "       [-13.20624058,   4.26322921],\n",
       "       [-20.25825084,  18.08750969],\n",
       "       [  2.80607217,  15.15208947],\n",
       "       [ 22.72895958, -22.57758849],\n",
       "       [ 13.47258175,  45.88647883],\n",
       "       [  2.25196854,  35.34771369],\n",
       "       [  0.37961339, -24.19629019],\n",
       "       [ 24.33065368,  34.29732811],\n",
       "       [-13.45513119, -36.96685273],\n",
       "       [-37.63676584,  40.00872384],\n",
       "       [  5.82788854, -15.64992398],\n",
       "       [ 18.75113569,  -2.44481871],\n",
       "       [-35.62363267,  21.38822935],\n",
       "       [-20.84943148, -34.80123172],\n",
       "       [-80.92657916,  -2.98129001],\n",
       "       [-20.48087136, -24.32156243],\n",
       "       [-19.36585829,   1.79094372],\n",
       "       [ -3.93376324, -99.52293268],\n",
       "       [-16.74975177, -11.03784277],\n",
       "       [ 11.41258801, -36.3129458 ],\n",
       "       [-24.25195622,  14.27678373],\n",
       "       [  2.90545312, -26.99763691],\n",
       "       [ -2.79241059,  52.79223336],\n",
       "       [-13.84843933, -36.74082088],\n",
       "       [ 47.93396709,  28.13672677],\n",
       "       [ 50.89966695,  20.30404833],\n",
       "       [ 30.68881511, -20.57479729],\n",
       "       [ 31.79539125, -11.03746082],\n",
       "       [ 45.62620454,   9.67165164],\n",
       "       [ 34.38445141,  12.61471468],\n",
       "       [ 22.2260703 ,  -4.80193236],\n",
       "       [-38.53460479,   2.1262844 ],\n",
       "       [  9.68842279, -24.57925211],\n",
       "       [  4.52595304,  34.88880169],\n",
       "       [ 27.86461482,  28.77558829],\n",
       "       [-17.7909776 , -76.88805062],\n",
       "       [-11.58196183, -23.83889246],\n",
       "       [-22.51129576,  51.54036135],\n",
       "       [ 28.96609315, -10.40652199],\n",
       "       [ -2.27436986,  -8.50106501],\n",
       "       [ -7.52048794, -41.30730061],\n",
       "       [ -6.68876012,  32.55392946],\n",
       "       [ 37.0094321 , -24.56843678],\n",
       "       [ 65.59977768,  40.46540128],\n",
       "       [  1.23806291, -48.77203231],\n",
       "       [ 10.02978187, -44.55108084],\n",
       "       [ 48.55128335, -49.6566633 ],\n",
       "       [  7.73776952, -24.11238874],\n",
       "       [  7.55770263, -19.88129603],\n",
       "       [-19.31466578,   7.4999719 ],\n",
       "       [ 24.01826448,  20.01697705],\n",
       "       [ 10.1926555 , -49.65445144],\n",
       "       [-35.28895906, -19.42873713],\n",
       "       [  9.02852505,   2.98130746],\n",
       "       [-49.18704208,  25.17645701],\n",
       "       [ 15.80170282,   2.82850209],\n",
       "       [ -6.20391893,  42.28663253],\n",
       "       [ 33.75904519,  -5.55268634],\n",
       "       [ 33.13991775,  21.11600315],\n",
       "       [ 14.46759281, -12.02265287],\n",
       "       [-27.84082851, -24.04875948],\n",
       "       [ 14.07414474, -42.57451647],\n",
       "       [-13.8516194 , -14.65922194],\n",
       "       [-21.76839865,  28.66001962],\n",
       "       [-20.74194329, -24.97797965],\n",
       "       [-17.98923475,   5.77388813],\n",
       "       [-14.24951474,  31.39523623],\n",
       "       [ -7.89447608, -25.26111808],\n",
       "       [ 14.47610353, -20.07325482],\n",
       "       [-13.88079951, -20.03473509],\n",
       "       [ -5.39508822,  22.17281704],\n",
       "       [ 35.80127334, -47.38131663],\n",
       "       [-16.12547758,  21.49265416]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c951c9",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea683a8d",
   "metadata": {},
   "source": [
    "The output of the decoder is calculated as the dot product of the latent variable $ \\mathbf{z} $ and the decoder weights $ \\mathbf{W}_{\\text{dec}} $, plus the decoder bias $ \\mathbf{b}_{\\text{dec}} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{o} = \\mathbf{z} \\mathbf{W}_{\\text{dec}} + \\mathbf{b}_{\\text{dec}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "979202ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(z, dec_w, dec_b):\n",
    "    \"\"\"\n",
    "    The decoder function that maps the latent space back to the input space.\n",
    "\n",
    "    :param z: Sampled latent variable.\n",
    "    :param dec_w: Decoder weights.\n",
    "    :param dec_b: Decoder biases.\n",
    "    :return: Reconstructed input.\n",
    "    \"\"\"\n",
    "    return np.dot(z, dec_w) + dec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae34d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_x = decoder(sample, dec_w, dec_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a7a856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.9086372 ,  27.55151299,  30.95954549, ...,  -2.28886593,\n",
       "         48.54909035,  26.07550394],\n",
       "       [ 13.98775391, -13.11149067,  27.07841003, ...,  29.80200605,\n",
       "         -5.11690448,  -7.04823013],\n",
       "       [ 18.34758026,  26.19219364,  37.7110792 , ...,   3.50937215,\n",
       "         49.71537239,  25.85048845],\n",
       "       ...,\n",
       "       [  6.79284933,  22.30429762,  14.59889524, ...,  -9.03899027,\n",
       "         34.80116383,  19.76771945],\n",
       "       [-27.56005445, -40.70138207, -56.71474558, ...,  -4.15795978,\n",
       "        -76.4437219 , -39.92852412],\n",
       "       [ 12.44007216,  18.49536024,  25.60617551, ...,   1.7755081 ,\n",
       "         34.6658773 ,  18.12288878]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5abb5f0",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d840a",
   "metadata": {},
   "source": [
    "The loss function for the VAE consists of two parts: the reconstruction loss and the KL divergence. The reconstruction loss is calculated as the mean squared error between the original input $x $ and the reconstructed input $ \\hat{x} $:\n",
    "\n",
    "$$\n",
    "\\text{recon loss} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{x}_i)^2\n",
    "$$\n",
    "\n",
    "The KL divergence loss is calculated using the mean $ \\mu $ and the standard deviation $ \\sigma $ of the latent space distribution:\n",
    "\n",
    "$$\n",
    "\\text{KL loss} = -\\frac{1}{2} \\sum_{i=1}^{n} \\left(1 + \\log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26b0c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, recon_x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Loss function for the VAE, combining reconstruction loss and KL divergence.\n",
    "\n",
    "    :param x: Original input data.\n",
    "    :param recon_x: Reconstructed input data.\n",
    "    :param mu: Mean of the latent space distribution.\n",
    "    :param sigma: Standard deviation of the latent space distribution.\n",
    "    :return: Total loss value.\n",
    "    \"\"\"\n",
    "    recon_loss = np.mean((x - recon_x) ** 2)\n",
    "    kl_loss = -0.5 * np.sum(1 + np.log(sigma**2) - mu**2 - sigma**2)\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb6a0b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(x, recon_x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "609ce956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83040.88028406404"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52610661",
   "metadata": {},
   "source": [
    "A training step in a Variational Autoencoder involves several stages. Given an input $ \\mathbf{x} $, the encoder generates parameters $ \\boldsymbol{\\mu} $ and $ \\boldsymbol{\\sigma} $ for the latent distribution:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu}, \\boldsymbol{\\sigma} = \\text{encoder}(\\mathbf{x}, \\mathbf{W}_{\\text{enc}}, \\mathbf{b}_{\\text{enc}})\n",
    "$$\n",
    "\n",
    "A latent variable $ \\mathbf{z} $ is then sampled from this distribution:\n",
    "\n",
    "$$\n",
    "\\mathbf{z} = \\text{sampling}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma})\n",
    "$$\n",
    "\n",
    "This latent variable is passed through the decoder to produce a reconstruction $ \\hat{\\mathbf{x}} $ of the original input:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{x}} = \\text{decoder}(\\mathbf{z}, \\mathbf{W}_{\\text{dec}}, \\mathbf{b}_{\\text{dec}})\n",
    "$$\n",
    "\n",
    "The loss for this training step is computed as a combination of the reconstruction error between $ \\mathbf{x} $ and $ \\hat{\\mathbf{x}} $, and a regularization term from the KL divergence between the latent distribution and the prior:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\text{loss function}(\\mathbf{x}, \\hat{\\mathbf{x}}, \\boldsymbol{\\mu}, \\boldsymbol{\\sigma})\n",
    "$$\n",
    "\n",
    "This loss $ \\mathcal{L} $ is then used to update the model parameters $ \\mathbf{W}_{\\text{enc}}, \\mathbf{b}_{\\text{enc}}, \\mathbf{W}_{\\text{dec}}, \\mathbf{b}_{\\text{dec}} $ through backpropagation and an optimization algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2ec8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x, enc_w, enc_b, dec_w, dec_b):\n",
    "    \"\"\"\n",
    "    A single training step for the VAE.\n",
    "\n",
    "    :param x: Input data for training.\n",
    "    :param enc_w: Encoder weights.\n",
    "    :param enc_b: Encoder biases.\n",
    "    :param dec_w: Decoder weights.\n",
    "    :param dec_b: Decoder biases.\n",
    "    :return: Loss value for the current training step.\n",
    "    \"\"\"\n",
    "    mu, sigma = encoder(x, enc_w, enc_b)\n",
    "    z = sampling(mu, sigma)\n",
    "    recon_x = decoder(z, dec_w, dec_b)\n",
    "    return loss_function(x, recon_x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37f003d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 84751.71519722772\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_dim = 784  # Example input dimension\n",
    "latent_dim = 2   # Example latent dimension\n",
    "enc_w, enc_b, dec_w, dec_b = initialize_weights(input_dim, latent_dim)\n",
    "x = np.random.randn(100, input_dim)  # Random input data\n",
    "loss = train_step(x, enc_w, enc_b, dec_w, dec_b)\n",
    "print(f\"Loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
