{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e22111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774d6b1",
   "metadata": {},
   "source": [
    "In the following code, the generator and discriminator are single-layer perceptrons (which is not practical for real-world data but is used here for simplicity). The generator's output is a deterministic function of its input noise and weights; in a more realistic scenario, you would use a non-linear activation function and multiple layers.\n",
    "\n",
    "The __train_step__ function at the final cell shows how the generator and discriminator might be updated during training, although the actual weight updates are not implemented here. Normally, you would use an optimization algorithm like gradient descent to adjust the weights based on the gradients of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea2d62",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80133cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "g_input_size = 1     # Size of random noise vector.\n",
    "g_hidden_size = 5    # Generator complexity.\n",
    "g_output_size = 1    # Size of generated data.\n",
    "\n",
    "d_input_size = 1     # Minibatch size - cardinality of distributions.\n",
    "d_hidden_size = 5    # Discriminator complexity.\n",
    "d_output_size = 1    # Single dimension for 'real' vs. 'fake' classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e0a632",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d48819",
   "metadata": {},
   "source": [
    "The sigmoid activation function $ \\sigma(x) $ is defined mathematically as:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x $ is the input to the function,\n",
    "- $ \\sigma(x) $ is the output between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d36554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2d5a4c",
   "metadata": {},
   "source": [
    "The generator in a Generative Adversarial Network takes a random noise vector $ \\mathbf{z} $ as input and uses it to generate fake data. The process involves two steps: first, transforming the noise vector using a weight matrix $ \\mathbf{W}_{g1} $, and second, applying the sigmoid activation function. The output is then transformed again by another weight matrix $ \\mathbf{W}_{g2} $ and passed through a sigmoid function to produce the final fake data $ \\mathbf{x}_{\\text{fake}} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{h} = \\sigma(\\mathbf{z} \\mathbf{W}_{g1})\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}_{\\text{fake}} = \\sigma(\\mathbf{h} \\mathbf{W}_{g2})\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ \\mathbf{z} $ is the generator input (random noise).\n",
    "- $ \\mathbf{W}_{g1} $ and $ \\mathbf{W}_{g2} $ are the weights for the first and second layers of the generator, respectively.\n",
    "- $ \\sigma $ represents the sigmoid activation function.\n",
    "- $ \\mathbf{h} $ is the hidden layer representation.\n",
    "- $ \\mathbf{x}_{\\text{fake}} $ is the generated fake data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4e03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def generate_fake_data(generator_input, generator_weights):\n",
    "    \"\"\"\n",
    "    The generator takes a random noise and uses it to generate fake data.\n",
    "    \n",
    "    :param generator_input: Random noise.\n",
    "    :param generator_weights: Weights for the generator model.\n",
    "    :return: Fake data.\n",
    "    \"\"\"\n",
    "    hidden_layer = np.dot(generator_input, generator_weights['g1'])\n",
    "    hidden_layer = sigmoid(hidden_layer)\n",
    "    output_layer = np.dot(hidden_layer, generator_weights['g2'])\n",
    "    fake_data = sigmoid(output_layer)\n",
    "    return fake_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d7f20e",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddc7cb4",
   "metadata": {},
   "source": [
    "The discriminator takes input data and processes it through layers with weights to classify the data as real or fake.\n",
    "\n",
    "1. The input data $ \\mathbf{x} $ is first transformed by a weight matrix $ \\mathbf{W}_{d1} $ and then passed through a sigmoid activation function:\n",
    "\n",
    "   $$ \\mathbf{h} = \\sigma(\\mathbf{x} \\mathbf{W}_{d1}) $$\n",
    "\n",
    "2. The output of this transformation $ \\mathbf{h} $ is then further transformed by another weight matrix $ \\mathbf{W}_{d2} $ and again passed through a sigmoid function to produce the final classification:\n",
    "\n",
    "   $$ \\text{classification} = \\sigma(\\mathbf{h} \\mathbf{W}_{d2}) $$\n",
    "\n",
    "Here:\n",
    "- $ \\mathbf{x} $ represents the input data to be classified.\n",
    "- $ \\mathbf{W}_{d1} $ and $ \\mathbf{W}_{d2} $ are the weight matrices of the first and second layers of the discriminator, respectively.\n",
    "- $ \\sigma $ is the sigmoid activation function.\n",
    "- $ \\mathbf{h} $ is the hidden layer representation.\n",
    "- The final output, `classification`, indicates the probability of the input data being real or fake as determined by the discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43458f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def discriminate_data(data, discriminator_weights):\n",
    "    \"\"\"\n",
    "    The discriminator takes data and tries to classify it as real or fake.\n",
    "    \n",
    "    :param data: Data to be classified.\n",
    "    :param discriminator_weights: Weights for the discriminator model.\n",
    "    :return: Classification probabilities.\n",
    "    \"\"\"\n",
    "    hidden_layer = np.dot(data, discriminator_weights['d1'])\n",
    "    hidden_layer = sigmoid(hidden_layer)\n",
    "    output_layer = np.dot(hidden_layer, discriminator_weights['d2'])\n",
    "    classification = sigmoid(output_layer)\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7f8ab",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b091235",
   "metadata": {},
   "source": [
    "The loss function for the discriminator in a Generative Adversarial Network is designed to measure its ability to distinguish between real and fake data. It consists of two parts: the loss for real data and the loss for fake data. Mathematically, this can be represented as follows:\n",
    "\n",
    "1. The loss for real data, where the discriminator output for real data is $ D(\\mathbf{x}_{\\text{real}}) $:\n",
    "\n",
    "   $$ \\text{loss}_{\\text{real}} = -\\sum_{i} \\log(D(\\mathbf{x}_{\\text{real}})_i) $$\n",
    "\n",
    "2. The loss for fake data, where the discriminator output for fake data is $ D(\\mathbf{x}_{\\text{fake}}) $:\n",
    "\n",
    "   $$ \\text{loss}_{\\text{fake}} = -\\sum_{i} \\log(1 - D(\\mathbf{x}_{\\text{fake}})_i) $$\n",
    "\n",
    "The total loss for the discriminator is the average of these two losses:\n",
    "\n",
    "$$ \\text{total loss} = \\frac{1}{2} \\left( \\text{loss}_{\\text{real}} + \\text{loss}_{\\text{fake}} \\right) $$\n",
    "\n",
    "Here, $ D(\\mathbf{x}) $ represents the discriminator's output probability that $ \\mathbf{x} $ is real, and the summation is over all instances in the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d236f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def loss_function(real_output, fake_output):\n",
    "    \"\"\"\n",
    "    The loss function for the discriminator to measure its performance.\n",
    "    \n",
    "    :param real_output: Discriminator output for real data.\n",
    "    :param fake_output: Discriminator output for fake data.\n",
    "    :return: Loss value.\n",
    "    \"\"\"\n",
    "    # Calculate the loss from the real data being classified as fake.\n",
    "    real_loss = -np.log(real_output)\n",
    "    # Calculate the loss from the fake data being classified as real.\n",
    "    fake_loss = -np.log(1 - fake_output)\n",
    "    # Total discriminator loss is the average of these losses.\n",
    "    total_loss = np.mean(real_loss + fake_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7489e",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0732e99e",
   "metadata": {},
   "source": [
    "In a Generative Adversarial Network, the weights for both the generator and the discriminator are typically initialized randomly. This initialization can be represented mathematically as follows:\n",
    "\n",
    "For the generator, we initialize two sets of weights, $ \\mathbf{W}_{g1} $ and $ \\mathbf{W}_{g2} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{g1} \\sim \\mathcal{N}(0, 1), \\quad \\mathbf{W}_{g1} \\in \\mathbb{R}^{\\text{g input size} \\times \\text{g hidden size}}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{W}_{g2} \\sim \\mathcal{N}(0, 1), \\quad \\mathbf{W}_{g2} \\in \\mathbb{R}^{\\text{g hidden size} \\times \\text{g output size}}\n",
    "$$\n",
    "\n",
    "For the discriminator, we initialize two sets of weights, $ \\mathbf{W}_{d1} $ and $ \\mathbf{W}_{d2} $:\n",
    "\n",
    "$$\n",
    "\\mathbf{W}_{d1} \\sim \\mathcal{N}(0, 1), \\quad \\mathbf{W}_{d1} \\in \\mathbb{R}^{\\text{d input size} \\times \\text{d hidden size}}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{W}_{d2} \\sim \\mathcal{N}(0, 1), \\quad \\mathbf{W}_{d2} \\in \\mathbb{R}^{\\text{d hidden size} \\times \\text{d output size}}\n",
    "$$\n",
    "\n",
    "Here, $ \\mathcal{N}(0, 1) $ indicates that the weights are drawn from a normal distribution with mean 0 and standard deviation 1. The dimensions of the weight matrices are determined by the size of the input layer, the size of the hidden layer, and the size of the output layer for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dec42871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "def initialize_weights():\n",
    "    \"\"\"\n",
    "    Initialize weights randomly for both generator and discriminator.\n",
    "    \n",
    "    :return: A tuple of dictionaries containing weights for both models.\n",
    "    \"\"\"\n",
    "    generator_weights = {\n",
    "        'g1': np.random.randn(g_input_size, g_hidden_size),\n",
    "        'g2': np.random.randn(g_hidden_size, g_output_size)\n",
    "    }\n",
    "    discriminator_weights = {\n",
    "        'd1': np.random.randn(d_input_size, d_hidden_size),\n",
    "        'd2': np.random.randn(d_hidden_size, d_output_size)\n",
    "    }\n",
    "    return generator_weights, discriminator_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e7c42",
   "metadata": {},
   "source": [
    "In a training step of a Generative Adversarial Network (GAN), the following processes occur:\n",
    "\n",
    "1. **Generation of Fake Data**: \n",
    "   The generator creates fake data $\\mathbf{x}_{\\text{fake}}$ by transforming a random noise vector $\\mathbf{z}$ using its current weights $\\mathbf{W}_g$:\n",
    "   $$ \\mathbf{x}_{\\text{fake}} = \\text{generate fake data}(\\mathbf{z}, \\mathbf{W}_g) $$\n",
    "\n",
    "\n",
    "\n",
    "2. **Discrimination of Real and Fake Data**: \n",
    "   The discriminator evaluates both real data $\\mathbf{x}_{\\text{real}}$ and fake data $\\mathbf{x}_{\\text{fake}}$, producing outputs $D(\\mathbf{x}_{\\text{real}})$ and $D(\\mathbf{x}_{\\text{fake}})$ respectively, using its current weights $\\mathbf{W}_d$:\n",
    "   $$ D(\\mathbf{x}_{\\text{real}}) = \\text{discriminate data}(\\mathbf{x}_{\\text{real}}, \\mathbf{W}_d) $$\n",
    "   $$ D(\\mathbf{x}_{\\text{fake}}) = \\text{discriminate data}(\\mathbf{x}_{\\text{fake}}, \\mathbf{W}_d) $$\n",
    "\n",
    "3. **Computation of the Discriminator's Loss**: \n",
    "   The loss for the discriminator $\\mathcal{L}_d$ is computed based on its ability to correctly classify real and fake data:\n",
    "   $$ \\mathcal{L}_d = \\text{loss function}(D(\\mathbf{x}_{\\text{real}}), D(\\mathbf{x}_{\\text{fake}})) $$\n",
    "\n",
    "In a full training cycle, the weights of both the generator and the discriminator ($\\mathbf{W}_g$ and $\\mathbf{W}_d$) would typically be updated based on this loss, usually employing a gradient descent algorithm. However, the weight update mechanism is not included in this simplified example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a618cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training step\n",
    "def train_step(real_data, generator_weights, discriminator_weights):\n",
    "    \"\"\"\n",
    "    A single training step for the GAN, consisting of:\n",
    "    \n",
    "    - Generating fake data.\n",
    "    - Training the discriminator to distinguish real from fake data.\n",
    "    - Updating the generator to produce better fake data.\n",
    "    \n",
    "    :param real_data: Real data to train the discriminator.\n",
    "    :param generator_weights: Current weights of the generator.\n",
    "    :param discriminator_weights: Current weights of the discriminator.\n",
    "    \"\"\"\n",
    "    # Generate fake data\n",
    "    random_noise = np.random.randn(g_input_size)\n",
    "    fake_data = generate_fake_data(random_noise, generator_weights)\n",
    "    \n",
    "    # Discriminate real and fake data\n",
    "    real_output = discriminate_data(real_data, discriminator_weights)\n",
    "    fake_output = discriminate_data(fake_data, discriminator_weights)\n",
    "    \n",
    "    # Compute loss for discriminator\n",
    "    d_loss = loss_function(real_output, fake_output)\n",
    "    \n",
    "    # Normally we would update the weights here using the loss.\n",
    "    # This would require implementing backpropagation and an optimization algorithm,\n",
    "    # which is beyond the scope of this simplified example.\n",
    "    \n",
    "    return d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e43efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator loss: 2.165026057870444\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights\n",
    "gen_weights, disc_weights = initialize_weights()\n",
    "\n",
    "# Generate some 'real' data (for demonstration purposes)\n",
    "real_data = np.random.randn(d_input_size)\n",
    "\n",
    "# Perform a training step\n",
    "loss = train_step(real_data, gen_weights, disc_weights)\n",
    "print(f\"Discriminator loss: {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
